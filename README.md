<img src="docs/rldb.png" align="right" width="20%"/>

# rldb

[![Build Status](https://travis-ci.com/seungjaeryanlee/rldb.svg?branch=master)](https://travis-ci.com/seungjaeryanlee/rldb)

![Environments tracked in rldb](https://img.shields.io/badge/environments-57-blue.svg)
![Papers tracked in rldb](https://img.shields.io/badge/papers-9-blue.svg)
![Algorithms tracked in rldb](https://img.shields.io/badge/algorithms-30-blue.svg)
![Entries tracked in rldb](https://img.shields.io/badge/entries-1586-blue.svg)

Database of state-of-the-art RL algorithms

## Examples

| Atari Breakout Scores                              | Atari Montezuma's Revenge Scores                                       |
|:--------------------------------------------------:|:----------------------------------------------------------------------:|
| ![Atari Breakout Scores](/docs/atari-breakout.png) | ![Atari Montezuma's Revenge Scores](/docs/atari-montezuma-revenge.png) |

## Papers

### Completed

- [x] [Proximal Policy Optimization Algorithms (Schulman et al., 2017)](https://arxiv.org/abs/1707.06347)
- [x] [Trust Region Policy Optimization (Schulman et al., 2015)](https://arxiv.org/abs/1502.05477)
- [x] [Exploration by Random Network Distillation (Burda et al., 2018)](https://arxiv.org/abs/1810.12894)
- [x] [Playing Atari with Deep Reinforcement Learning (Mnih et al., 2013)](https://arxiv.org/abs/1312.5602)
- [x] [Deep Recurrent Q-Learning for Partially Observable MDPs (Hausknecht and Stone, 2015)](https://arxiv.org/abs/1507.06527)
- [x] [Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015)](https://arxiv.org/abs/1511.06581)
- [x] [Deep Reinforcement Learning with Double Q-learning (Hasselt et al., 2015)](https://arxiv.org/abs/1509.06461)
- [x] [Prioritized Experience Replay (Schaul et al., 2015)](https://arxiv.org/abs/1511.05952)
- [x] [Human-level Control through Deep Reinforcement Learning (Mnih et al., 2015)](https://deepmind.com/research/dqn/)

### To Be Added
